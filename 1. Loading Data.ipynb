{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to use popular preloaded datasets to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "features = digits.data\n",
    "target = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "       15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "       12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "        0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "       10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, scikit-learn comes with some common datasets we can quickly load.\n",
    "These datasets are often called “toy” datasets because they are far smaller and cleaner\n",
    "than a dataset we would see in the real world. Some popular sample datasets in scikitlearn\n",
    "are:\n",
    "load_boston\n",
    "Contains 503 observations on Boston housing prices. It is a good dataset for\n",
    "exploring regression algorithms.\n",
    "load_iris\n",
    "Contains 150 observations on the measurements of Iris flowers. It is a good dataset\n",
    "for exploring classification algorithms.\n",
    "load_digits\n",
    "Contains 1,797 observations from images of handwritten digits. It is a good dataset\n",
    "for teaching image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Simulated Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn offers many methods for creating simulated data. Of those, three methods\n",
    "are particularly usefulWhen we want a dataset designed to be used with linear regression, make_regression is a good choice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features,target,coefficients = make_regression(\n",
    "    n_samples=100,\n",
    "    n_features=3,\n",
    "    n_informative=3,\n",
    "    n_targets=1,\n",
    "    bias=0.0,\n",
    "    effective_rank=None,\n",
    "    tail_strength=0.5,\n",
    "    noise=0.0,\n",
    "    shuffle=True,\n",
    "    coef=True,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix\n",
      " [[ 1.29322588 -0.61736206 -0.11044703]\n",
      " [-2.793085    0.36633201  1.93752881]\n",
      " [ 0.80186103 -0.18656977  0.0465673 ]]\n",
      "Target Vector\n",
      " [-10.37865986  25.5124503   19.67705609]\n"
     ]
    }
   ],
   "source": [
    "print('Feature Matrix\\n', features[:3])\n",
    "print('Target Vector\\n', target[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are interested in creating a simulated dataset for classification, we can use\n",
    "make_classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix\n",
      " [[ 1.06354768 -1.42632219  1.02163151]\n",
      " [ 0.23156977  1.49535261  0.33251578]\n",
      " [ 0.15972951  0.83533515 -0.40869554]]\n",
      "Target Vector\n",
      " [1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Load library\n",
    "from sklearn.datasets import make_classification\n",
    "# Generate features matrix and target vector\n",
    "features, target = make_classification(n_samples = 100,\n",
    "n_features = 3,\n",
    "n_informative = 3,\n",
    "n_redundant = 0,\n",
    "n_classes = 2,\n",
    "weights = [.25, .75],\n",
    "random_state = 1)\n",
    "# View feature matrix and target vector\n",
    "print('Feature Matrix\\n', features[:3])\n",
    "print('Target Vector\\n', target[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if we want a dataset designed to work well with clustering techniques, scikitlearn\n",
    "offers make_blobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix\n",
      " [[ -1.22685609   3.25572052]\n",
      " [ -9.57463218  -4.38310652]\n",
      " [-10.71976941  -4.20558148]]\n",
      "Target Vector\n",
      " [0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Load library\n",
    "from sklearn.datasets import make_blobs\n",
    "# Generate feature matrix and target vector\n",
    "features, target = make_blobs(n_samples = 100,\n",
    "n_features = 2,\n",
    "centers = 3,\n",
    "cluster_std = 0.5,\n",
    "shuffle = True,\n",
    "random_state = 1)\n",
    "# View feature matrix and target vector\n",
    "print('Feature Matrix\\n', features[:3])\n",
    "print('Target Vector\\n', target[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As might be apparent from the solutions, make_regression returns a feature matrix\n",
    "of float values and a target vector of float values, while make_classification and\n",
    "make_blobs return a feature matrix of float values and a target vector of integers representing\n",
    "membership in a class.\n",
    "scikit-learn’s simulated datasets offer extensive options to control the type of data\n",
    "generated. scikit-learn’s documentation contains a full description of all the parameters,\n",
    "but a few are worth noting.\n",
    "In make_regression and make_classification, n_informative determines the\n",
    "number of features that are used to generate the target vector. If n_informative is less\n",
    "than the total number of features (n_features), the resulting dataset will have redundant\n",
    "features that can be identified through feature selection techniques.\n",
    "In addition, make_classification contains a weights parameter that allows us to\n",
    "simulate datasets with imbalanced classes. For example, weights = [.25, .75]\n",
    "would return a dataset with 25% of observations belonging to one class and 75% of\n",
    "observations belonging to a second class.For make_blobs, the centers parameter determines the number of clusters generated.\n",
    "Using the matplotlib visualization library, we can visualize the clusters generated by\n",
    "make_blobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
